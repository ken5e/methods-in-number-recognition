{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNForNumberRecognition.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LoIh3t0mFlz"
      },
      "source": [
        "# **Importing modules**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPL_mFSt3Ri7"
      },
      "source": [
        "# Import Tensorflow 2.0\n",
        "import tensorflow as tf \n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras import layers, models\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DWq1lrJ_zsW"
      },
      "source": [
        "#%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ekhxyERmMDq"
      },
      "source": [
        "Split the data and scale them by dividing 255\n",
        "Also add an extra dimension for inputing into the Convolution layer  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbUKDaYQ41Og",
        "outputId": "2887b177-e4d0-4fae-c222-1dc35f04371e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data() #Y : name of the number ; X : picture of that number in 28x28\n",
        "X_train = X_train/255.0\n",
        "X_test = X_test/255.0\n",
        "X_train = tf.expand_dims(X_train, -1)\n",
        "X_test = tf.expand_dims(X_test, -1)\n",
        "print(tf.shape(X_train))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "tf.Tensor([60000    28    28     1], shape=(4,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqLHhTNumzuz"
      },
      "source": [
        "# **Structure of the neural network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rljspY3k2-oM",
        "outputId": "e141f1b6-1682-4a3d-c884-7fd09402df27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (4, 4), strides = 1, activation='relu', input_shape=(28, 28,1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "#model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10))\n",
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 25, 25, 32)        544       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 10, 10, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                102464    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 122,154\n",
            "Trainable params: 122,154\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VBtoVtEdgPt"
      },
      "source": [
        "# **Train the model manually**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRtHbVlWQSPg",
        "outputId": "3dc12d38-163f-41e8-b0ea-cc48922a94fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "batch_size = 12\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2)\n",
        "for epoch in range(1):\n",
        "  print(f'\\n{epoch+1}')\n",
        "\n",
        "  for idx in tqdm(range(0, X_train.shape[0], batch_size)):\n",
        "  # First grab a batch of training data and convert the input images to tensors\n",
        "    (images, labels) = (X_train[idx:idx+batch_size], Y_train[idx:idx+batch_size])\n",
        "  #images = tf.convert_to_tensor(images, dtype=tf.float32)\n",
        "\n",
        "  # GradientTape to record differentiation operations\n",
        "    with tf.GradientTape() as tape:\n",
        "      logits = model(images)\n",
        "      loss_value = tf.keras.backend.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "  # Backpropagation\n",
        "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 19/5000 [00:00<00:26, 184.74it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [00:24<00:00, 201.02it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqQF3PC1m5dm"
      },
      "source": [
        "# **Train the model with 'fit'**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22B16_nJmiEf",
        "outputId": "bc6d7ced-8a4d-4798-9a13-50172af2bdc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, Y_train, epochs=10, \n",
        "                    validation_data=(X_test, Y_test))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1341 - accuracy: 0.9591 - val_loss: 0.0483 - val_accuracy: 0.9858\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0452 - accuracy: 0.9861 - val_loss: 0.0291 - val_accuracy: 0.9909\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0321 - accuracy: 0.9898 - val_loss: 0.0311 - val_accuracy: 0.9900\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0242 - accuracy: 0.9920 - val_loss: 0.0295 - val_accuracy: 0.9907\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0191 - accuracy: 0.9938 - val_loss: 0.0312 - val_accuracy: 0.9899\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0155 - accuracy: 0.9948 - val_loss: 0.0269 - val_accuracy: 0.9912\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0109 - accuracy: 0.9963 - val_loss: 0.0257 - val_accuracy: 0.9916\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0105 - accuracy: 0.9964 - val_loss: 0.0360 - val_accuracy: 0.9894\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0080 - accuracy: 0.9973 - val_loss: 0.0255 - val_accuracy: 0.9931\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0085 - accuracy: 0.9971 - val_loss: 0.0343 - val_accuracy: 0.9915\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-vI_57em_tk"
      },
      "source": [
        "# **Make a prediction using the testing data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2QOx61wquH8",
        "outputId": "ff7d9dab-0742-49db-f5c5-9b2307d9a71e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "  import numpy as np\n",
        "  data = tf.expand_dims(X_test[12], 0)\n",
        "  prediction = model.predict(data)\n",
        "\n",
        "  if np.argmax(prediction[0]) == Y_test[12]:\n",
        "    no = str(Y_test[12])\n",
        "    img = tf.squeeze(X_test[12], [2])\n",
        "    imgplot = plt.imshow(img)\n",
        "    print(f'prediction:{no}')\n",
        "  else:\n",
        "    print('cannot do')\n",
        "    print(f'{np.argmax(prediction[0])}')\n",
        "    img = tf.squeeze(X_test[12], [2])\n",
        "    imgplot = plt.imshow(img)\n",
        "  \n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prediction:9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}